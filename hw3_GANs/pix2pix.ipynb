{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "! kaggle datasets download -p data vikramtiwari/pix2pix-dataset \n",
    "! cd ./data && unzip pix2pix-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"./data/facades/facades\")\n",
    "RUN_EXP_DIR = Path(\"./runs\")\n",
    "ARTIFACTS_DIR = RUN_EXP_DIR / \"artifacts\"\n",
    "CHECKPOINT_DIR = RUN_EXP_DIR / \"checkpoints\"\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "N_EPOCHS = 300\n",
    "G_LR = 2e-5\n",
    "D_LR = 2e-5\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, data_dir: Path, stage=\"train\", transform=None) -> None:\n",
    "        self._images_paths = list((data_dir / stage).iterdir())\n",
    "        self._transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = cv2.imread(str(self._images_paths[idx]))\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # h, w, _ = img.shape\n",
    "        # img_target = img[:, : w // 2]\n",
    "        # img_input = img[:, w // 2:]\n",
    "        \n",
    "        img = Image.open(self._images_paths[idx])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h))\n",
    "        img_B = img.crop((w / 2, 0, w, h))\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        if self._transform:\n",
    "            # transformed = self._transform(image=img_target, image_input=img_input)\n",
    "            # img_target, img_input = transformed[\"image\"], transformed[\"image_input\"]\n",
    "            img_target = self._transform(img_A)\n",
    "            img_input = self._transform(img_B)\n",
    "\n",
    "        return img_input, img_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images_paths)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    # A.Resize(height=256, width=256),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.CoarseDropout(max_holes=8, max_height=8, max_width=8),\n",
    "    ToTensorV2()\n",
    "], additional_targets={'image_input': 'image'})\n",
    "\n",
    "transform = A.Compose([\n",
    "    # A.Resize(height=256, width=256),\n",
    "    ToTensorV2()\n",
    "], additional_targets={'image_input': 'image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FacadesDataset(DATA_DIR, \"train\", train_transform)\n",
    "valid_ds = FacadesDataset(DATA_DIR, \"val\", transform)\n",
    "test_ds = FacadesDataset(DATA_DIR, \"test\", transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.unet = smp.Unet(\n",
    "            \"resnet34\",\n",
    "            in_channels=3,\n",
    "            classes=3,\n",
    "            encoder_depth=5,\n",
    "            encoder_weights=\"imagenet\",\n",
    "            decoder_channels=(256, 128, 64, 32, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.unet(x)\n",
    "        return out\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "#     def __init__(self, input_nc=3, output_nc=3, nf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "#         \"\"\"Construct a Unet generator\n",
    "#         Parameters:\n",
    "#             input_nc (int)  -- the number of channels in input images\n",
    "#             output_nc (int) -- the number of channels in output images\n",
    "#             num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "#                                 image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "#             nf (int)       -- the number of filters in the last conv layer\n",
    "#             norm_layer      -- normalization layer\n",
    "#         We construct the U-Net from the innermost layer to the outermost layer.\n",
    "#         It is a recursive process.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         # construct unet structure\n",
    "#         unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        \n",
    "#         # add intermediate layers with ngf * 8 filters\n",
    "#         unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "#         unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "#         unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        \n",
    "#         # gradually reduce the number of filters from nf * 8 to nf\n",
    "#         unet_block = UnetSkipConnectionBlock(nf * 4, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "#         unet_block = UnetSkipConnectionBlock(nf * 2, nf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "#         unet_block = UnetSkipConnectionBlock(nf, nf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "#         self.model = UnetSkipConnectionBlock(output_nc, nf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         \"\"\"Standard forward\"\"\"\n",
    "#         return self.model(input)\n",
    "    \n",
    "\n",
    "# class UnetSkipConnectionBlock(nn.Module):\n",
    "#     \"\"\"Defines the Unet submodule with skip connection.\n",
    "#         X -------------------identity----------------------\n",
    "#         |-- downsampling -- |submodule| -- upsampling --|\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "#                  submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "#         \"\"\"Construct a Unet submodule with skip connections.\n",
    "#         Parameters:\n",
    "#             outer_nc (int) -- the number of filters in the outer conv layer\n",
    "#             inner_nc (int) -- the number of filters in the inner conv layer\n",
    "#             input_nc (int) -- the number of channels in input images/features\n",
    "#             submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "#             outermost (bool)    -- if this module is the outermost module\n",
    "#             innermost (bool)    -- if this module is the innermost module\n",
    "#             norm_layer          -- normalization layer\n",
    "#             use_dropout (bool)  -- if use dropout layers.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.outermost = outermost\n",
    "#         if input_nc is None:\n",
    "#             input_nc = outer_nc\n",
    "#         downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "#                              stride=2, padding=1, bias=False)\n",
    "#         downrelu = nn.LeakyReLU(0.2, True)\n",
    "#         downnorm = norm_layer(inner_nc)\n",
    "#         uprelu = nn.ReLU(True)\n",
    "#         upnorm = norm_layer(outer_nc)\n",
    "\n",
    "#         if outermost:\n",
    "#             upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "#                                         kernel_size=4, stride=2,\n",
    "#                                         padding=1)\n",
    "#             down = [downconv]\n",
    "#             up = [uprelu, upconv, nn.Tanh()]\n",
    "#             model = down + [submodule] + up\n",
    "#         elif innermost:\n",
    "#             upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "#                                         kernel_size=4, stride=2,\n",
    "#                                         padding=1, bias=False)\n",
    "#             down = [downrelu, downconv]\n",
    "#             up = [uprelu, upconv, upnorm]\n",
    "#             model = down + up\n",
    "#         else:\n",
    "#             upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "#                                         kernel_size=4, stride=2,\n",
    "#                                         padding=1, bias=False)\n",
    "#             down = [downrelu, downconv, downnorm]\n",
    "#             up = [uprelu, upconv, upnorm]\n",
    "\n",
    "#             if use_dropout:\n",
    "#                 model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "#             else:\n",
    "#                 model = down + [submodule] + up\n",
    "\n",
    "#         self.model = nn.Sequential(*model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.outermost:\n",
    "#             return self.model(x)\n",
    "#         else:   # add skip connections\n",
    "#             return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=False),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=False),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw), nn.Sigmoid()]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(DEVICE)\n",
    "discriminator = Discriminator(6, 64).to(DEVICE)\n",
    "\n",
    "G_optimizer = torch.optim.Adam(generator.parameters(), lr=G_LR, betas=(0.5, 0.999))\n",
    "D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=D_LR, betas=(0.5, 0.999))\n",
    "\n",
    "adversarial_loss = nn.BCELoss() \n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "def generator_loss(generated_image, target_img, G, real_target):\n",
    "    gen_loss = adversarial_loss(G, real_target)\n",
    "    l1_l = l1_loss(generated_image, target_img)\n",
    "    gen_total_loss = gen_loss + (100 * l1_l)\n",
    "    return gen_total_loss\n",
    "\n",
    "def discriminator_loss(output, label):\n",
    "    return adversarial_loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/and/projects/MADE/3sem/made_2022_advanced_dl_and_rl/hw3_GANs/pix2pix.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/and/projects/MADE/3sem/made_2022_advanced_dl_and_rl/hw3_GANs/pix2pix.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# compute gradients and run optimizer step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/and/projects/MADE/3sem/made_2022_advanced_dl_and_rl/hw3_GANs/pix2pix.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m D_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/and/projects/MADE/3sem/made_2022_advanced_dl_and_rl/hw3_GANs/pix2pix.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m D_total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/and/projects/MADE/3sem/made_2022_advanced_dl_and_rl/hw3_GANs/pix2pix.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m D_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/and/projects/MADE/3sem/made_2022_advanced_dl_and_rl/hw3_GANs/pix2pix.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Train generator with real labels\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/made/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/made/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D_epoch_losses, G_epoch_losses= [], []\n",
    "for epoch in tqdm(range(1, N_EPOCHS + 1)): \n",
    "    D_loss_list, G_loss_list = [], []\n",
    "   \n",
    "    for input_img, target_img in train_loader:\n",
    "        input_img = input_img.to(DEVICE)\n",
    "        target_img = target_img.to(DEVICE)\n",
    "       \n",
    "        # ground truth labels real and fake\n",
    "        real_target = torch.ones(input_img.size(0), 1, 30, 30, requires_grad=True).to(DEVICE)\n",
    "        fake_target = torch.zeros(input_img.size(0), 1, 30, 30, requires_grad=True).to(DEVICE)\n",
    "    \n",
    "        # generator forward pass\n",
    "        generated_img = generator(input_img.float())\n",
    "        \n",
    "        # train discriminator with fake/generated images\n",
    "        D_fake_input = torch.cat((input_img, generated_img), 1)\n",
    "        D_fake_output = discriminator(D_fake_input.detach())\n",
    "        D_fake_loss = discriminator_loss(D_fake_output, fake_target)\n",
    "        \n",
    "        # train discriminator with real images\n",
    "        D_real_input = torch.cat((input_img, target_img), 1)        \n",
    "        D_real_output = discriminator(D_real_input)\n",
    "        D_real_loss = discriminator_loss(D_real_output,  real_target)\n",
    "\n",
    "        # average discriminator loss\n",
    "        D_total_loss = (D_real_loss + D_fake_loss) / 2\n",
    "        D_loss_list.append(D_total_loss.item())\n",
    "        \n",
    "        # compute gradients and run optimizer step\n",
    "        D_optimizer.zero_grad()\n",
    "        D_total_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Train generator with real labels\n",
    "        G_fake_input = torch.cat((input_img, generated_img), 1)\n",
    "        G = discriminator(G_fake_input)\n",
    "        G_loss = generator_loss(generated_img, target_img, G, real_target)                                 \n",
    "        G_loss_list.append(G_loss.item())\n",
    "        \n",
    "        # compute gradients and run optimizer step\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "            \n",
    "    D_epoch_losses.append(np.mean(D_loss_list))\n",
    "    G_epoch_losses.append(np.mean(G_loss_list))\n",
    "     \n",
    "    print(f\"[Epoch {epoch}/{N_EPOCHS}] D_loss: {D_epoch_losses[-1]} G_loss: {G_epoch_losses[-1]}\")\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        torch.save(generator.state_dict(), f\"./artifacts/generator_epoch_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"./artifacts/discriminator_epoch_{epoch}.pth\")\n",
    "\n",
    "        for input_img, target_img in valid_loader:\n",
    "            input_img = input_img.to(DEVICE)\n",
    "            target_img = target_img.to(DEVICE)\n",
    "            generated_img = generator(input_img)\n",
    "            save_image(\n",
    "                flatten(\n",
    "                    [[input_img[i], target_img[i], generated_img[i]] for i in range(3)]\n",
    "                ), \n",
    "                f\"./pix2pix_sample_{epoch}.png\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    clear_output()\n",
    "    plt.plot(D_epoch_losses)\n",
    "    plt.plot(G_epoch_losses)\n",
    "    plt.legend(['Discriminator loss', 'Generator loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "made",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86c658836fd1cd990cb7feace1b3fb4dfeb93dea87b3a33e91181f6d4506db52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
